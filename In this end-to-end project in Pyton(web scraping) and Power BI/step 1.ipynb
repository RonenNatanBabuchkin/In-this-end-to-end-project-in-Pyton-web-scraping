{"cells":[{"source":"# **#Introduction : In this end-to-end  project.** \n## we have used cricket 2023 in Test matches  data to build insights on a best 11 players team that we can assemble from the earth that can go and play with aliens. \n## We creating data web scraping to collect data from espncricinfo website then we performed some data transformation and cleaning in pandas, followed by building dashboards in power bi.\n### Steps: \n### step 1 - creating data web scraping to collect data.\n### step 2 - performed some data transformation and cleaning in pandas.\n### step 3 - building dashboards in power bi.","metadata":{},"cell_type":"markdown","id":"c61cb719-8d51-46c6-be8e-3169ebfb9f07"},{"source":"# step 1 - creating data web scraping to collect data. ","metadata":{},"cell_type":"markdown","id":"d188bfc2-3cca-4b7c-87eb-c9d386b7ae26"},{"source":"from bs4 import BeautifulSoup # Web scraping package\nimport requests # To pull urls\nimport pandas as pd # For working with a dataframe\n","metadata":{"executionCancelledAt":null,"executionTime":224,"lastExecutedAt":1687679304335,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from bs4 import BeautifulSoup # Web scraping package\nimport requests # To pull urls\nimport pandas as pd # For working with a dataframe\n"},"cell_type":"code","id":"4b5a208f-26bb-47e9-bb9e-4978e0ed2dc8","execution_count":1,"outputs":[]},{"source":"## This is a function that generates a table of matches and creates three lists:\n## 1. url_links - containing the url links for game cards\n## 2. match_IDs - containing the ID match\n## 3. match_names - containing the name of the match.","metadata":{},"cell_type":"markdown","id":"00d649dc-20a2-4481-8b47-879875b896a1"},{"source":"def Creating_df_matche(rows): \n    # Creating lists for each column\n    tabel_match = []\n    \n    # Creating list for ulr and dic for matchID match\n    url_link = []\n    list_match_ID = []\n    list_match = []\n    \n    # Iterate over each row and extract the data\n    for row in rows:\n        # Find all data each row\n        columns = row.find_all(\"td\")\n        # Adding values to each list\n        tabel_match.append({\n            \"match\" : columns[0].text.strip() + \" VS \" + columns[1].text.strip(),\n            \"winer\" : columns[2].text.strip(),\n            \"margin\" : columns[3].text.strip(),\n            \"ground\" : columns[4].text.strip(),\n            \"match_date\" : columns[5].text.strip(),\n            \"match_ID\" : columns[6].text.strip()\n        })\n        url_link.append(\"https://www.espncricinfo.com\" + columns[6].find('a').get('href')) \n        list_match_ID.append(columns[6].text.strip())\n        list_match.append(columns[0].text.strip() + \" VS \" + columns[1].text.strip())\n\n    \n    \n    # Creating match_results_df\n    tabel_match_df = pd.DataFrame(tabel_match) \n\n    return tabel_match_df ,url_link ,list_match_ID, list_match\n\n    \n    ","metadata":{"executionCancelledAt":null,"executionTime":14,"lastExecutedAt":1687679306287,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"def Creating_df_matche(rows): \n    # Creating lists for each column\n    tabel_match = []\n    \n    # Creating list for ulr and dic for matchID match\n    url_link = []\n    list_match_ID = []\n    list_match = []\n    \n    # Iterate over each row and extract the data\n    for row in rows:\n        # Find all data each row\n        columns = row.find_all(\"td\")\n        # Adding values to each list\n        tabel_match.append({\n            \"match\" : columns[0].text.strip() + \" VS \" + columns[1].text.strip(),\n            \"winer\" : columns[2].text.strip(),\n            \"margin\" : columns[3].text.strip(),\n            \"ground\" : columns[4].text.strip(),\n            \"match_date\" : columns[5].text.strip(),\n            \"match_ID\" : columns[6].text.strip()\n        })\n        url_link.append(\"https://www.espncricinfo.com\" + columns[6].find('a').get('href')) \n        list_match_ID.append(columns[6].text.strip())\n        list_match.append(columns[0].text.strip() + \" VS \" + columns[1].text.strip())\n\n    \n    \n    # Creating match_results_df\n    tabel_match_df = pd.DataFrame(tabel_match) \n\n    return tabel_match_df ,url_link ,list_match_ID, list_match\n\n    \n    ","collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false}},"cell_type":"code","id":"ca7badc5-81d9-4a0f-b6d3-b9fb39317cb2","execution_count":2,"outputs":[]},{"source":"## This function takes in an URL, match ID, and match name as inputs and generates two tables:\n## 1. table_batting - containing details of players who batted in the match\n## 2. table_player_batting - containing personal details of players who batted in the match","metadata":{},"cell_type":"markdown","id":"23903f22-0059-4ca1-96eb-194478a60553"},{"source":"def Creating_df_tabel_batting(url, match_ID, match):\n    # Specify the URL of the webpage you want to scrape\n    # Send an HTTP GET request to the URL\n    response = requests.get(url)\n    if response.status_code == 200:\n        html = response.text\n        soup = BeautifulSoup(html, 'html.parser')\n\n        # Finding all tables with a given suffix and we know they are found in index 0 and 1\n        tables = soup.select('div > table.ci-scorecard-table')\n        # In these tables there are empty rows according to the HTML code, \n        # so we filtered for rows that have at least 11 columns as we need.\n        firstInningRows = list(filter(lambda tr: len(tr.find_all(\"td\")) >= 8, tables[0].select('tbody > tr')))\n        secondInningsRows = list(filter(lambda tr: len(tr.find_all(\"td\")) >= 8, tables[1].select('tbody > tr')))\n\n\n        # Creating two list, first for players URL and second for batting Summary\n        list_url_plyear = [] \n        battingSummary = []\n        # A loop that builds these lists from the first table on the site\n        for index, element in enumerate(firstInningRows):\n            tds = element.find_all('td')\n            battingSummary.append({\n                \"match\": match,\n                \"match_ID\" : match_ID,\n                \"teamInnings\": \"Innings 1\",\n                \"battingPos\": index+1,\n                \"batsmanName\": tds[0].find('a').text,\n                \"Out/Not Out\": \"Not Out\" if tds[1].find('span') == None else \"Out\" , \n                \"runs\": tds[2].find('strong').text,\n                \"balls\": tds[3].text,\n                \"4s\": tds[5].text,\n                \"6s\": tds[6].text,\n                \"SR\": tds[7].text\n            })\n            list_url_plyear.append(\"https://www.espncricinfo.com\" + tds[0].find('a').get('href'))\n        \n        # A loop that builds these lists from the second table on the site\n        for index, element in enumerate(secondInningsRows):\n            tds = element.find_all('td')\n            battingSummary.append({\n                \"match\": match,\n                \"match_ID\" : match_ID,\n                \"teamInnings\": \"Innings 2\",\n                \"battingPos\": index+1,\n                \"batsmanName\": tds[0].find('a').text,\n                \"Out/Not Out\": \"Not Out\" if tds[1].find('span') == None else \"Out\" ,\n                \"runs\": tds[2].find('strong').text,\n                \"balls\": tds[3].text,\n                \"4s\": tds[5].text,\n                \"6s\": tds[6].text,\n                \"SR\": tds[7].text\n            })\n            list_url_plyear.append(\"https://www.espncricinfo.com\" + tds[0].find('a').get('href'))\n\n            \n        # Build a list of all the players with their personal details,\n        # We send the list of links to the method and it returns a dictionary of players    \n        tabel_palyer_dic = Creating_df_tabel_player(list_url_plyear)         \n        \n        tabel_player_df = pd.DataFrame(tabel_palyer_dic)      \n        tabel_Batting = pd.DataFrame(battingSummary) \n        \n        return tabel_Batting , tabel_player_df","metadata":{"executionCancelledAt":null,"executionTime":55,"lastExecutedAt":1687679529426,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"def Creating_df_tabel_batting(url, match_ID, match):\n    # Specify the URL of the webpage you want to scrape\n    # Send an HTTP GET request to the URL\n    response = requests.get(url)\n    if response.status_code == 200:\n        html = response.text\n        soup = BeautifulSoup(html, 'html.parser')\n\n        # Finding all tables with a given suffix and we know they are found in index 0 and 1\n        tables = soup.select('div > table.ci-scorecard-table')\n        # In these tables there are empty rows according to the HTML code, \n        # so we filtered for rows that have at least 11 columns as we need.\n        firstInningRows = list(filter(lambda tr: len(tr.find_all(\"td\")) >= 8, tables[0].select('tbody > tr')))\n        secondInningsRows = list(filter(lambda tr: len(tr.find_all(\"td\")) >= 8, tables[1].select('tbody > tr')))\n\n\n        # Creating two list, first for players URL and second for batting Summary\n        list_url_plyear = [] \n        battingSummary = []\n        # A loop that builds these lists from the first table on the site\n        for index, element in enumerate(firstInningRows):\n            tds = element.find_all('td')\n            battingSummary.append({\n                \"match\": match,\n                \"match_ID\" : match_ID,\n                \"teamInnings\": \"Innings 1\",\n                \"battingPos\": index+1,\n                \"batsmanName\": tds[0].find('a').text,\n                \"Out/Not Out\": \"Not Out\" if tds[1].find('span') == None else \"Out\" , \n                \"runs\": tds[2].find('strong').text,\n                \"balls\": tds[3].text,\n                \"4s\": tds[5].text,\n                \"6s\": tds[6].text,\n                \"SR\": tds[7].text\n            })\n            list_url_plyear.append(\"https://www.espncricinfo.com\" + tds[0].find('a').get('href'))\n        \n        # A loop that builds these lists from the second table on the site\n        for index, element in enumerate(secondInningsRows):\n            tds = element.find_all('td')\n            battingSummary.append({\n                \"match\": match,\n                \"match_ID\" : match_ID,\n                \"teamInnings\": \"Innings 2\",\n                \"battingPos\": index+1,\n                \"batsmanName\": tds[0].find('a').text,\n                \"Out/Not Out\": \"Not Out\" if tds[1].find('span') == None else \"Out\" ,\n                \"runs\": tds[2].find('strong').text,\n                \"balls\": tds[3].text,\n                \"4s\": tds[5].text,\n                \"6s\": tds[6].text,\n                \"SR\": tds[7].text\n            })\n            list_url_plyear.append(\"https://www.espncricinfo.com\" + tds[0].find('a').get('href'))\n\n            \n        # Build a list of all the players with their personal details,\n        # We send the list of links to the method and it returns a dictionary of players    \n        tabel_palyer_dic = Creating_df_tabel_player(list_url_plyear)         \n        \n        tabel_player_df = pd.DataFrame(tabel_palyer_dic)      \n        tabel_Batting = pd.DataFrame(battingSummary) \n        \n        return tabel_Batting , tabel_player_df","collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false}},"cell_type":"code","id":"371f6327-b02f-4a67-9074-557c5ba79703","execution_count":4,"outputs":[]},{"source":"## This function takes in an URL, match ID, and match name as inputs and generates two tables:\n## 1. table_bowling - containing details of players who bowled in the match\n## 2. table_player_bowling - containing personal details of players who bowled in the match ","metadata":{},"cell_type":"markdown","id":"fe7e0606-f0ac-4312-bdf6-ad4561a92348"},{"source":"def Creating_df_tabel_bowling(url,match_ID, match):\n    # Specify the URL of the webpage you want to scrape\n    # Send an HTTP GET request to the URL\n    response = requests.get(url)\n    if response.status_code == 200:\n        html = response.text\n        soup = BeautifulSoup(html, 'html.parser')\n\n        # Finding all tables with a given suffix and we know they are found in index 1 and 3\n        tables = soup.select('div > table.ds-table') \n        # In these tables there are empty rows according to the HTML code, \n        # so we filtered for rows that have at least 11 columns as we need.\n        firstInningRows = list(filter(lambda tr: len(tr.find_all(\"td\")) >= 11, tables[1].select('tbody > tr')))\n        secondInningsRows = list(filter(lambda tr: len(tr.find_all(\"td\")) >= 11, tables[3].select('tbody > tr'))) \n\n        # Creating two list, first for players URL and second for bowling Summary\n        list_url_plyear = []    \n        bowlingSummary = []\n        # A loop that builds these lists from the first table on the site\n        for index, element in enumerate(firstInningRows):\n            tds = element.find_all('td')\n            bowlingSummary.append({\n                \"match\": match,\n                \"match_ID\" : match_ID,\n                \"teamInnings\": \"Innings 1\",\n                \"bowlerName\": tds[0].find('a').text,\n                \"overs\": tds[1].text,\n                \"maiden\": tds[2].text,\n                \"runs\": tds[3].text,\n                \"wickets\": tds[4].text,\n                \"economy\": tds[5].text,\n                \"0s\": tds[6].text,\n                \"4s\": tds[7].text,\n                \"6s\": tds[8].text,\n                \"wides\": tds[9].text,\n                \"noBalls\": tds[10].text\n            })\n            list_url_plyear.append(\"https://www.espncricinfo.com\" + tds[0].find('a').get('href'))\n\n        # A loop that builds these lists from the second table on the site    \n        for index, element in enumerate(secondInningsRows):\n            tds = element.find_all('td')\n            bowlingSummary.append({\n                \"match\": match,\n                \"match_ID\" : match_ID,\n                \"teamInnings\": \"Innings 2\",\n                \"bowlerName\": tds[0].find('a').text,\n                \"overs\": tds[1].text,\n                \"maiden\": tds[2].text,\n                \"runs\": tds[3].text,\n                \"wickets\": tds[4].text,\n                \"economy\": tds[5].text,\n                \"0s\": tds[6].text,\n                \"4s\": tds[7].text,\n                \"6s\": tds[8].text,\n                \"wides\": tds[9].text,\n                \"noBalls\": tds[10].text\n            })\n            list_url_plyear.append(\"https://www.espncricinfo.com\" + tds[0].find('a').get('href'))\n\n            \n        # Build a list of all the players with their personal details,\n        # We send the list of links to the method and it returns a dictionary of players   \n        tabel_palyer_dic = Creating_df_tabel_player(list_url_plyear)  \n        \n        \n        tabel_player_df = pd.DataFrame(tabel_palyer_dic)   \n        table_bowling = pd.DataFrame(bowlingSummary)\n        \n        return table_bowling, tabel_player_df","metadata":{"executionCancelledAt":null,"executionTime":54,"lastExecutedAt":1687679637282,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"def Creating_df_tabel_bowling(url,match_ID, match):\n    # Specify the URL of the webpage you want to scrape\n    # Send an HTTP GET request to the URL\n    response = requests.get(url)\n    if response.status_code == 200:\n        html = response.text\n        soup = BeautifulSoup(html, 'html.parser')\n\n        # Finding all tables with a given suffix and we know they are found in index 1 and 3\n        tables = soup.select('div > table.ds-table') \n        # In these tables there are empty rows according to the HTML code, \n        # so we filtered for rows that have at least 11 columns as we need.\n        firstInningRows = list(filter(lambda tr: len(tr.find_all(\"td\")) >= 11, tables[1].select('tbody > tr')))\n        secondInningsRows = list(filter(lambda tr: len(tr.find_all(\"td\")) >= 11, tables[3].select('tbody > tr'))) \n\n        # Creating two list, first for players URL and second for bowling Summary\n        list_url_plyear = []    \n        bowlingSummary = []\n        # A loop that builds these lists from the first table on the site\n        for index, element in enumerate(firstInningRows):\n            tds = element.find_all('td')\n            bowlingSummary.append({\n                \"match\": match,\n                \"match_ID\" : match_ID,\n                \"teamInnings\": \"Innings 1\",\n                \"bowlerName\": tds[0].find('a').text,\n                \"overs\": tds[1].text,\n                \"maiden\": tds[2].text,\n                \"runs\": tds[3].text,\n                \"wickets\": tds[4].text,\n                \"economy\": tds[5].text,\n                \"0s\": tds[6].text,\n                \"4s\": tds[7].text,\n                \"6s\": tds[8].text,\n                \"wides\": tds[9].text,\n                \"noBalls\": tds[10].text\n            })\n            list_url_plyear.append(\"https://www.espncricinfo.com\" + tds[0].find('a').get('href'))\n\n        # A loop that builds these lists from the second table on the site    \n        for index, element in enumerate(secondInningsRows):\n            tds = element.find_all('td')\n            bowlingSummary.append({\n                \"match\": match,\n                \"match_ID\" : match_ID,\n                \"teamInnings\": \"Innings 2\",\n                \"bowlerName\": tds[0].find('a').text,\n                \"overs\": tds[1].text,\n                \"maiden\": tds[2].text,\n                \"runs\": tds[3].text,\n                \"wickets\": tds[4].text,\n                \"economy\": tds[5].text,\n                \"0s\": tds[6].text,\n                \"4s\": tds[7].text,\n                \"6s\": tds[8].text,\n                \"wides\": tds[9].text,\n                \"noBalls\": tds[10].text\n            })\n            list_url_plyear.append(\"https://www.espncricinfo.com\" + tds[0].find('a').get('href'))\n\n            \n        # Build a list of all the players with their personal details,\n        # We send the list of links to the method and it returns a dictionary of players   \n        tabel_palyer_dic = Creating_df_tabel_player(list_url_plyear)  \n        \n        \n        tabel_player_df = pd.DataFrame(tabel_palyer_dic)   \n        table_bowling = pd.DataFrame(bowlingSummary)\n        \n        return table_bowling, tabel_player_df","collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false}},"cell_type":"code","id":"eecc46fa-91dd-4180-a6cf-6054253d5536","execution_count":6,"outputs":[]},{"source":"## This is a Python function that receives a list of URLs to player websites and builds a tabel of all the players' details.","metadata":{},"cell_type":"markdown","id":"a168541f-054b-40c8-b0a0-c84b03714afa"},{"source":"def  Creating_df_tabel_player(list_url_player):\n    # Creating a list contin info for players. \n    player_info = [] \n    \n    # This is a loop that runs along all the web links of players and builds a list \n    for url in list_url_player:\n        response = requests.get(url)\n        if response.status_code == 200:\n            html = response.text\n            soup = BeautifulSoup(html, 'html.parser') \n\n            # info from balck card in website   \n            info_balck_card = soup.select('div > div.ds-p-0')[0]\n            # info from white card in website\n            info_white_card = soup.select('div > div.ds-p-4')[0]\n\n            # x contin info team and role, \n            x = info_balck_card.find_all('span')\n            # y contion info batting Style and bowling Style\n            y = info_white_card.find_all('p')    \n\n            player_info.append({\n                \"name\" :info_balck_card.find('h1').text ,\n                \"team\" : x[0].text ,\n                \"playingRole\" : x[2].text,\n                \"battingStyle\" : y[7].text ,\n                \"bowlingStyle\" : y[9].text\n            })  \n\n    return player_info","metadata":{"executionCancelledAt":null,"executionTime":12,"lastExecutedAt":1687679641207,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"def  Creating_df_tabel_player(list_url_player):\n    # Creating a list contin info for players. \n    player_info = [] \n    \n    # This is a loop that runs along all the web links of players and builds a list \n    for url in list_url_player:\n        response = requests.get(url)\n        if response.status_code == 200:\n            html = response.text\n            soup = BeautifulSoup(html, 'html.parser') \n\n            # info from balck card in website   \n            info_balck_card = soup.select('div > div.ds-p-0')[0]\n            # info from white card in website\n            info_white_card = soup.select('div > div.ds-p-4')[0]\n\n            # x contin info team and role, \n            x = info_balck_card.find_all('span')\n            # y contion info batting Style and bowling Style\n            y = info_white_card.find_all('p')    \n\n            player_info.append({\n                \"name\" :info_balck_card.find('h1').text ,\n                \"team\" : x[0].text ,\n                \"playingRole\" : x[2].text,\n                \"battingStyle\" : y[7].text ,\n                \"bowlingStyle\" : y[9].text\n            })  \n\n    return player_info","collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false}},"cell_type":"code","id":"4e209063-2f4c-4309-925f-2cbfcd33a1df","execution_count":7,"outputs":[]},{"source":"## This is a Python function that receives a URL to a website with a summarized match table and returns four dataframes:\n## 1. Match table\n## 2. Batting table\n## 3. Bowling table\n## 4. Player table","metadata":{},"cell_type":"markdown","id":"add9fa34-6a38-48bd-84bb-5852f34c829e"},{"source":"# Specify the URL of the webpage you want to scrape\nurl = \"https://www.espncricinfo.com/records/year/team-match-results/2023-2023/test-matches-1\"\n\n# Send an HTTP GET request to the URL\nresponse = requests.get(url)\nif response.status_code == 200:\n    html = response.text\n    soup = BeautifulSoup(html, 'html.parser')\n\n    # Find the table containing the match results\n    table = soup.find(\"table\")\n\n    # Find all rows in the table except the header row\n    rows = table.find_all(\"tr\")[1:]\n\n    # create a table Matche results \n    table_match_results, url_link ,list_match_ID, list_match  = Creating_df_matche(rows)\n\n    # Creating a table of batting A table detailing players who batting the first link\n    table_batting, table_player_batting = Creating_df_tabel_batting(url_link[0],list_match_ID[0],list_match[0])\n    \n    # Creating a table of bowling A table detailing players who bowling the first link\n    table_bowling, table_player_bowling = Creating_df_tabel_bowling(url_link[0],list_match_ID[0],list_match[0])\n    \n    # A vertical connection of details of players who batting and who bowling.\n    table_player = pd.concat([table_player_batting,table_player_bowling],ignore_index = True )\n\n    # A loop that connects all the tables according to the url link to match scorecard\n    for x in range(1,len(url_link)):\n        # Creating a table of batting A table detailing players who batting the other links\n        table_batting_temp , table_player_batting_temp = Creating_df_tabel_batting(url_link[x],list_match_ID[x],list_match[x])\n        # Creating a table of bowling A table detailing players who bowling the other links\n        table_bowling_temp , table_player_bowling_temp = Creating_df_tabel_bowling(url_link[x],list_match_ID[x],list_match[x])\n        \n        # Vertical connection of tables with batting\n        table_batting = pd.concat([table_batting, table_batting_temp], ignore_index = True)\n        # Vertical connection of tables with bowling \n        table_bowling = pd.concat([table_bowling, table_bowling_temp], ignore_index = True)\n        # Vertical connection of tables with details of players who batting and who bowling\n        table_player = pd.concat([table_player, table_player_batting_temp, table_player_bowling_temp], ignore_index = True) \n        \n    # Creating CSV files Go to step 2 of the project    \n    file_name1 = 'datasets/table_match_results.csv'\n    file_name2 = 'datasets/table_batting.csv' \n    file_name3 = 'datasets/table_bowling.csv'\n    file_name4 = 'datasets/table_player.csv' \n\n    table_match_results.to_csv(file_name1, index=False)\n    table_batting.to_csv(file_name2, index=False)\n    table_bowling.to_csv(file_name3, index=False)\n    table_player.to_csv(file_name4, index=False)\n","metadata":{"executionCancelledAt":null,"executionTime":203042,"lastExecutedAt":1687679852654,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Specify the URL of the webpage you want to scrape\nurl = \"https://www.espncricinfo.com/records/year/team-match-results/2023-2023/test-matches-1\"\n\n# Send an HTTP GET request to the URL\nresponse = requests.get(url)\nif response.status_code == 200:\n    html = response.text\n    soup = BeautifulSoup(html, 'html.parser')\n\n    # Find the table containing the match results\n    table = soup.find(\"table\")\n\n    # Find all rows in the table except the header row\n    rows = table.find_all(\"tr\")[1:]\n\n    # create a table Matche results \n    table_match_results, url_link ,list_match_ID, list_match  = Creating_df_matche(rows)\n\n    # Creating a table of batting A table detailing players who batting the first link\n    table_batting, table_player_batting = Creating_df_tabel_batting(url_link[0],list_match_ID[0],list_match[0])\n    \n    # Creating a table of bowling A table detailing players who bowling the first link\n    table_bowling, table_player_bowling = Creating_df_tabel_bowling(url_link[0],list_match_ID[0],list_match[0])\n    \n    # A vertical connection of details of players who batting and who bowling.\n    table_player = pd.concat([table_player_batting,table_player_bowling],ignore_index = True )\n\n    # A loop that connects all the tables according to the url link to match scorecard\n    for x in range(1,len(url_link)):\n         # Creating a table of batting A table detailing players who batting the other links\n        table_batting_temp , table_player_batting_temp = Creating_df_tabel_batting(url_link[x],list_match_ID[x],list_match[x])\n        # Creating a table of bowling A table detailing players who bowling the other links\n        table_bowling_temp , table_player_bowling_temp = Creating_df_tabel_bowling(url_link[x],list_match_ID[x],list_match[x])\n        \n        # Vertical connection of tables with batting\n        table_batting = pd.concat([table_batting, table_batting_temp], ignore_index = True)\n        # Vertical connection of tables with bowling \n        table_bowling = pd.concat([table_bowling, table_bowling_temp], ignore_index = True)\n        # Vertical connection of tables with details of players who batting and who bowling\n        table_player = pd.concat([table_player, table_player_batting_temp, table_player_bowling_temp], ignore_index = True) \n        \n    # Creating CSV files Go to step 2 of the project    \n    file_name1 = 'datasets/table_match_results.csv'\n    file_name2 = 'datasets/table_batting.csv' \n    file_name3 = 'datasets/table_bowling.csv'\n    file_name4 = 'datasets/table_player.csv' \n\n    table_match_results.to_csv(file_name1, index=False)\n    table_batting.to_csv(file_name2, index=False)\n    table_bowling.to_csv(file_name3, index=False)\n    table_player.to_csv(file_name4, index=False)\n","collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false}},"cell_type":"code","id":"509a4026-b352-4bbc-ba8a-7c365a5b96f8","execution_count":8,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"editor":"DataCamp Workspace"},"nbformat":4,"nbformat_minor":5}